{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "\n",
    "def stats(rewards):\n",
    "    print(\"Mean reward: \", np.mean(rewards))\n",
    "    rewards_100 = []\n",
    "    for i in range(100, len(rewards) + 1):\n",
    "        rewards_100.append(np.mean(rewards[i-100:i]))\n",
    "    #print(\"Max 100 rewards mean: \", np.max(rewards_100))\n",
    "    #re = np.argmax(rewards_100)\n",
    "    #print(\"Max 100 rewards from episode: %d, to episode: %d\" % (re, re + 99))\n",
    "    plt.plot(rewards_100)\n",
    "    plt.xlabel('episodes')\n",
    "    plt.ylabel('rewards')\n",
    "    plt.show()\n",
    "    \n",
    "def tolerant_mean(arrs):\n",
    "    lens = [len(i) for i in arrs]\n",
    "    arr = np.ma.empty((np.max(lens),len(arrs)))\n",
    "    arr.mask = True\n",
    "    for idx, l in enumerate(arrs):\n",
    "        arr[:len(l),idx] = l\n",
    "    return arr.mean(axis = -1), arr.std(axis=-1)\n",
    "    \n",
    "def train(num_iterations=10):\n",
    "    env = gym.make('CartPole-v0')\n",
    "    env._max_episode_steps = 1000\n",
    "    max_num_episodes = 2000\n",
    "    checkpoint = 400\n",
    "    replay_experience_maxlen = 50000\n",
    "    batch_size = 64\n",
    "    #num_iterations = 10\n",
    "    #final_episodes = []\n",
    "\n",
    "    learning_rate = 0.001\n",
    "    decay = 0.99\n",
    "    training_rewards = []\n",
    "        \n",
    "\n",
    "    augs = ['ras', None, 'gn']\n",
    "    final_rewards = []\n",
    "        \n",
    "    for aug in augs:\n",
    "        curr_rewards = []\n",
    "        final_episodes = []\n",
    "        for i in range(num_iterations):\n",
    "            print(\"----------------ITERATION: \", i, aug)\n",
    "            tf.compat.v1.reset_default_graph()\n",
    "            X = tf.placeholder(dtype=tf.float32, shape=(None, 4))\n",
    "            y = tf.placeholder(dtype=tf.float32, shape=(None, 2))\n",
    "            net = tf.contrib.layers.fully_connected(X, 15)\n",
    "            Q = tf.contrib.layers.fully_connected(net, 2, activation_fn=None)\n",
    "            mse = tf.contrib.losses.mean_squared_error(y, Q)\n",
    "            train_step = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(mse)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            sess = tf.InteractiveSession()\n",
    "            tf.global_variables_initializer().run()\n",
    "\n",
    "            # Initialize empty experiences\n",
    "            replay_experience = deque(maxlen=replay_experience_maxlen)\n",
    "            for episode in range(max_num_episodes):\n",
    "                state = env.reset()\n",
    "                epsilon = 1./((episode/50) + 10)\n",
    "                done = False\n",
    "                episode_reward = 0\n",
    "                while not done:\n",
    "                    # Calculate Q(s, a) for all a\n",
    "                    Q_s_A = Q.eval(feed_dict={X: state.reshape((1, 4))})\n",
    "\n",
    "                    # Choose action based on epsilon-greedy policy\n",
    "                    if np.random.random_sample() < epsilon:\n",
    "                        action = env.action_space.sample()\n",
    "                    else:\n",
    "                        action = np.argmax(Q_s_A[0])\n",
    "\n",
    "                    # Perform action\n",
    "                    next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "                    # Append final reward for each episode\n",
    "                    episode_reward += reward\n",
    "\n",
    "                    # Change 0 reward to -1 to learn more from punishment\n",
    "                    if done:\n",
    "                        reward = -1.0\n",
    "\n",
    "                    # Save experience\n",
    "                    replay_experience.append([state, action, reward, next_state, done])\n",
    "\n",
    "                    # Switch to next state\n",
    "                    state = next_state\n",
    "\n",
    "                    # Do training if replay_experience contains enough sample > batch_size\n",
    "                    if len(replay_experience) > batch_size:\n",
    "                        \n",
    "\n",
    "                        ## 1- Sample from replay experience\n",
    "                        batch = random.sample(replay_experience, batch_size)\n",
    "                        states = np.vstack([x[0] for x in batch])\n",
    "\n",
    "                        actions = np.array([x[1] for x in batch])\n",
    "                        rewards = np.array([x[2] for x in batch])\n",
    "                        next_states = np.vstack([x[3] for x in batch])\n",
    "                        if aug == 'ras':\n",
    "                            low = np.random.uniform(0.6, 0.8)\n",
    "                            high = np.random.uniform(1.2, 1.4)\n",
    "                            scaling = np.random.uniform(low, high)\n",
    "                            next_states *= scaling\n",
    "                            states *= scaling\n",
    "                        elif aug == 'gn':\n",
    "                            mean = np.mean(states)\n",
    "                            std = np.std(states)\n",
    "                            noise = np.random.normal(mean, std)\n",
    "                            states += noise\n",
    "                            next_states += noise\n",
    "\n",
    "                        episodes_done = np.array([x[4] for x in batch])\n",
    "                        target_Q = Q.eval(feed_dict={X: states})\n",
    "                        target_Q[range(batch_size), actions] = rewards + decay * np.max(Q.eval(feed_dict={X: next_states}), axis=1) * ~episodes_done\n",
    "                        train_step.run(feed_dict={X: states, y: target_Q})\n",
    "\n",
    "                if (episode + 1) % checkpoint == 0:\n",
    "                    print(\"Episode: %d\" % (episode))\n",
    "\n",
    "\n",
    "            print(\"Play 10 times with optimal policy\")\n",
    "            avg_rewards = []\n",
    "            for i in range(10):\n",
    "                state = env.reset()\n",
    "                done = False\n",
    "                total_reward = 0\n",
    "                while not done:\n",
    "                    state, reward, done, _ = env.step(np.argmax(Q.eval(feed_dict={X: state.reshape((1, 4))})))\n",
    "                    #print(state, reward, done)\n",
    "                    total_reward += reward\n",
    "                    #env.render()\n",
    "                avg_rewards.append(total_reward)\n",
    "                print(\"Iteration: %d, Total Reward: %d\" % (i, total_reward))\n",
    "            final_rewards.append([np.mean(avg_rewards), np.std(avg_rewards)])\n",
    "    return final_rewards\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------ITERATION:  0 ras\n",
      "Episode: 399\n",
      "Episode: 799\n",
      "Episode: 1199\n",
      "Episode: 1599\n",
      "Episode: 1999\n",
      "Play 10 times with optimal policy\n",
      "Iteration: 0, Total Reward: 110\n",
      "Iteration: 1, Total Reward: 110\n",
      "Iteration: 2, Total Reward: 10\n",
      "Iteration: 3, Total Reward: 9\n",
      "Iteration: 4, Total Reward: 10\n",
      "Iteration: 5, Total Reward: 109\n",
      "Iteration: 6, Total Reward: 10\n",
      "Iteration: 7, Total Reward: 112\n",
      "Iteration: 8, Total Reward: 111\n",
      "Iteration: 9, Total Reward: 115\n",
      "----------------ITERATION:  1 ras\n",
      "Episode: 399\n",
      "Episode: 799\n",
      "Episode: 1199\n",
      "Episode: 1599\n",
      "Episode: 1999\n",
      "Play 10 times with optimal policy\n",
      "Iteration: 0, Total Reward: 150\n",
      "Iteration: 1, Total Reward: 154\n",
      "Iteration: 2, Total Reward: 150\n",
      "Iteration: 3, Total Reward: 152\n",
      "Iteration: 4, Total Reward: 152\n",
      "Iteration: 5, Total Reward: 148\n",
      "Iteration: 6, Total Reward: 165\n",
      "Iteration: 7, Total Reward: 146\n",
      "Iteration: 8, Total Reward: 153\n",
      "Iteration: 9, Total Reward: 152\n",
      "----------------ITERATION:  2 ras\n",
      "Episode: 399\n",
      "Episode: 799\n",
      "Episode: 1199\n",
      "Episode: 1599\n",
      "Episode: 1999\n",
      "Play 10 times with optimal policy\n",
      "Iteration: 0, Total Reward: 1000\n",
      "Iteration: 1, Total Reward: 1000\n",
      "Iteration: 2, Total Reward: 1000\n",
      "Iteration: 3, Total Reward: 1000\n",
      "Iteration: 4, Total Reward: 1000\n",
      "Iteration: 5, Total Reward: 1000\n",
      "Iteration: 6, Total Reward: 1000\n",
      "Iteration: 7, Total Reward: 1000\n",
      "Iteration: 8, Total Reward: 1000\n",
      "Iteration: 9, Total Reward: 1000\n",
      "----------------ITERATION:  3 ras\n",
      "Episode: 399\n",
      "Episode: 799\n",
      "Episode: 1199\n",
      "Episode: 1599\n",
      "Episode: 1999\n",
      "Play 10 times with optimal policy\n",
      "Iteration: 0, Total Reward: 1000\n",
      "Iteration: 1, Total Reward: 1000\n",
      "Iteration: 2, Total Reward: 1000\n",
      "Iteration: 3, Total Reward: 1000\n",
      "Iteration: 4, Total Reward: 1000\n",
      "Iteration: 5, Total Reward: 1000\n",
      "Iteration: 6, Total Reward: 1000\n",
      "Iteration: 7, Total Reward: 1000\n",
      "Iteration: 8, Total Reward: 1000\n",
      "Iteration: 9, Total Reward: 1000\n",
      "----------------ITERATION:  4 ras\n",
      "Episode: 399\n",
      "Episode: 799\n",
      "Episode: 1199\n",
      "Episode: 1599\n",
      "Episode: 1999\n",
      "Play 10 times with optimal policy\n",
      "Iteration: 0, Total Reward: 1000\n",
      "Iteration: 1, Total Reward: 1000\n",
      "Iteration: 2, Total Reward: 1000\n",
      "Iteration: 3, Total Reward: 1000\n",
      "Iteration: 4, Total Reward: 1000\n",
      "Iteration: 5, Total Reward: 1000\n",
      "Iteration: 6, Total Reward: 1000\n",
      "Iteration: 7, Total Reward: 1000\n",
      "Iteration: 8, Total Reward: 1000\n",
      "Iteration: 9, Total Reward: 1000\n",
      "----------------ITERATION:  0 None\n",
      "Episode: 399\n",
      "Episode: 799\n",
      "Episode: 1199\n",
      "Episode: 1599\n",
      "Episode: 1999\n",
      "Play 10 times with optimal policy\n",
      "Iteration: 0, Total Reward: 10\n",
      "Iteration: 1, Total Reward: 11\n",
      "Iteration: 2, Total Reward: 10\n",
      "Iteration: 3, Total Reward: 10\n",
      "Iteration: 4, Total Reward: 9\n",
      "Iteration: 5, Total Reward: 11\n",
      "Iteration: 6, Total Reward: 10\n",
      "Iteration: 7, Total Reward: 10\n",
      "Iteration: 8, Total Reward: 9\n",
      "Iteration: 9, Total Reward: 8\n",
      "----------------ITERATION:  1 None\n",
      "Episode: 399\n",
      "Episode: 799\n",
      "Episode: 1199\n",
      "Episode: 1599\n",
      "Episode: 1999\n",
      "Play 10 times with optimal policy\n",
      "Iteration: 0, Total Reward: 11\n",
      "Iteration: 1, Total Reward: 35\n",
      "Iteration: 2, Total Reward: 12\n",
      "Iteration: 3, Total Reward: 13\n",
      "Iteration: 4, Total Reward: 34\n",
      "Iteration: 5, Total Reward: 34\n",
      "Iteration: 6, Total Reward: 12\n",
      "Iteration: 7, Total Reward: 31\n",
      "Iteration: 8, Total Reward: 34\n",
      "Iteration: 9, Total Reward: 12\n",
      "----------------ITERATION:  2 None\n",
      "Episode: 399\n",
      "Episode: 799\n",
      "Episode: 1199\n",
      "Episode: 1599\n",
      "Episode: 1999\n",
      "Play 10 times with optimal policy\n",
      "Iteration: 0, Total Reward: 162\n",
      "Iteration: 1, Total Reward: 137\n",
      "Iteration: 2, Total Reward: 148\n",
      "Iteration: 3, Total Reward: 135\n",
      "Iteration: 4, Total Reward: 149\n",
      "Iteration: 5, Total Reward: 165\n",
      "Iteration: 6, Total Reward: 153\n",
      "Iteration: 7, Total Reward: 166\n",
      "Iteration: 8, Total Reward: 151\n",
      "Iteration: 9, Total Reward: 143\n",
      "----------------ITERATION:  3 None\n",
      "Episode: 399\n",
      "Episode: 799\n",
      "Episode: 1199\n",
      "Episode: 1599\n",
      "Episode: 1999\n",
      "Play 10 times with optimal policy\n",
      "Iteration: 0, Total Reward: 1000\n",
      "Iteration: 1, Total Reward: 1000\n",
      "Iteration: 2, Total Reward: 411\n",
      "Iteration: 3, Total Reward: 1000\n",
      "Iteration: 4, Total Reward: 467\n",
      "Iteration: 5, Total Reward: 875\n",
      "Iteration: 6, Total Reward: 930\n",
      "Iteration: 7, Total Reward: 1000\n",
      "Iteration: 8, Total Reward: 1000\n",
      "Iteration: 9, Total Reward: 1000\n",
      "----------------ITERATION:  4 None\n",
      "Episode: 399\n",
      "Episode: 799\n",
      "Episode: 1199\n",
      "Episode: 1599\n",
      "Episode: 1999\n",
      "Play 10 times with optimal policy\n",
      "Iteration: 0, Total Reward: 13\n",
      "Iteration: 1, Total Reward: 11\n",
      "Iteration: 2, Total Reward: 13\n",
      "Iteration: 3, Total Reward: 13\n",
      "Iteration: 4, Total Reward: 11\n",
      "Iteration: 5, Total Reward: 11\n",
      "Iteration: 6, Total Reward: 12\n",
      "Iteration: 7, Total Reward: 11\n",
      "Iteration: 8, Total Reward: 12\n",
      "Iteration: 9, Total Reward: 11\n",
      "----------------ITERATION:  0 gn\n",
      "Episode: 399\n",
      "Episode: 799\n",
      "Episode: 1199\n",
      "Episode: 1599\n",
      "Episode: 1999\n",
      "Play 10 times with optimal policy\n",
      "Iteration: 0, Total Reward: 125\n",
      "Iteration: 1, Total Reward: 117\n",
      "Iteration: 2, Total Reward: 111\n",
      "Iteration: 3, Total Reward: 170\n",
      "Iteration: 4, Total Reward: 147\n",
      "Iteration: 5, Total Reward: 190\n",
      "Iteration: 6, Total Reward: 198\n",
      "Iteration: 7, Total Reward: 195\n",
      "Iteration: 8, Total Reward: 130\n",
      "Iteration: 9, Total Reward: 202\n",
      "----------------ITERATION:  1 gn\n",
      "Episode: 399\n",
      "Episode: 799\n",
      "Episode: 1199\n",
      "Episode: 1599\n",
      "Episode: 1999\n",
      "Play 10 times with optimal policy\n",
      "Iteration: 0, Total Reward: 107\n",
      "Iteration: 1, Total Reward: 129\n",
      "Iteration: 2, Total Reward: 164\n",
      "Iteration: 3, Total Reward: 103\n",
      "Iteration: 4, Total Reward: 179\n",
      "Iteration: 5, Total Reward: 117\n",
      "Iteration: 6, Total Reward: 225\n",
      "Iteration: 7, Total Reward: 147\n",
      "Iteration: 8, Total Reward: 108\n",
      "Iteration: 9, Total Reward: 191\n",
      "----------------ITERATION:  2 gn\n",
      "Episode: 399\n",
      "Episode: 799\n",
      "Episode: 1199\n",
      "Episode: 1599\n",
      "Episode: 1999\n",
      "Play 10 times with optimal policy\n",
      "Iteration: 0, Total Reward: 110\n",
      "Iteration: 1, Total Reward: 134\n",
      "Iteration: 2, Total Reward: 125\n",
      "Iteration: 3, Total Reward: 110\n",
      "Iteration: 4, Total Reward: 176\n",
      "Iteration: 5, Total Reward: 105\n",
      "Iteration: 6, Total Reward: 80\n",
      "Iteration: 7, Total Reward: 114\n",
      "Iteration: 8, Total Reward: 194\n",
      "Iteration: 9, Total Reward: 256\n",
      "----------------ITERATION:  3 gn\n",
      "Episode: 399\n",
      "Episode: 799\n",
      "Episode: 1199\n",
      "Episode: 1599\n",
      "Episode: 1999\n",
      "Play 10 times with optimal policy\n",
      "Iteration: 0, Total Reward: 255\n",
      "Iteration: 1, Total Reward: 192\n",
      "Iteration: 2, Total Reward: 182\n",
      "Iteration: 3, Total Reward: 151\n",
      "Iteration: 4, Total Reward: 213\n",
      "Iteration: 5, Total Reward: 157\n",
      "Iteration: 6, Total Reward: 202\n",
      "Iteration: 7, Total Reward: 165\n",
      "Iteration: 8, Total Reward: 177\n",
      "Iteration: 9, Total Reward: 175\n",
      "----------------ITERATION:  4 gn\n",
      "Episode: 399\n",
      "Episode: 799\n",
      "Episode: 1199\n",
      "Episode: 1599\n",
      "Episode: 1999\n",
      "Play 10 times with optimal policy\n",
      "Iteration: 0, Total Reward: 112\n",
      "Iteration: 1, Total Reward: 77\n",
      "Iteration: 2, Total Reward: 156\n",
      "Iteration: 3, Total Reward: 84\n",
      "Iteration: 4, Total Reward: 115\n",
      "Iteration: 5, Total Reward: 122\n",
      "Iteration: 6, Total Reward: 145\n",
      "Iteration: 7, Total Reward: 87\n",
      "Iteration: 8, Total Reward: 150\n",
      "Iteration: 9, Total Reward: 176\n"
     ]
    }
   ],
   "source": [
    "rewards = train(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[70.6, 49.70754469896899], [152.2, 4.833218389437828], [1000.0, 0.0], [1000.0, 0.0], [1000.0, 0.0], [9.8, 0.8717797887081348], [22.8, 10.85172797300043], [150.9, 10.367738422626218], [868.3, 218.65180081581767], [11.8, 0.8717797887081348], [158.5, 34.575280186861825], [147.0, 39.5145542806698], [140.4, 50.227880703848136], [186.9, 29.152872928752668], [122.4, 31.978742939646644]]\n",
      "15\n",
      "[  70.6  152.2 1000.  1000.  1000. ]\n",
      "644.5600000000001 436.0874297660963\n",
      "212.71999999999997 332.03066966772803\n",
      "151.04 21.407344534061203\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"y_ras, _ = tolerant_mean(rewards[0])\\nstats(y_ras)\\ny_base, _ = tolerant_mean(rewards[1])\\nstats(y_base)\\ny_gn, _ = tolerant_mean(rewards[2])\\nstats(y_gn)\\nplt.plot(np.arange(len(y_gn))+1, y_gn, color='red', label='Gaussian Noise')\\nplt.plot(np.arange(len(y_ras))+1, y_ras, color='blue', label='Random Amplitude Scaling')\\nplt.plot(np.arange(len(y_base))+1, y_base, color='green', label='Baseline')\\nplt.xlabel('epsidoes')\\nplt.ylabel('reward')\\nplt.legend()\\nplt.show()\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(rewards)\n",
    "print(len(rewards))\n",
    "\n",
    "np_rewards = np.array(rewards)\n",
    "\n",
    "print(np_rewards[:5,0])\n",
    "ras_mean = np.mean(np_rewards[:5,0])\n",
    "ras_std = np.std(np_rewards[:5,0])\n",
    "\n",
    "print(ras_mean, ras_std)\n",
    "\n",
    "base_mean = np.mean(np_rewards[5:10,0])\n",
    "base_std = np.std(np_rewards[5:10,0])\n",
    "\n",
    "print(base_mean, base_std)\n",
    "\n",
    "gn_mean = np.mean(np_rewards[10:,0])\n",
    "gn_std = np.std(np_rewards[10:,0])\n",
    "\n",
    "print(gn_mean, gn_std)\n",
    "\n",
    "'''y_ras, _ = tolerant_mean(rewards[0])\n",
    "stats(y_ras)\n",
    "y_base, _ = tolerant_mean(rewards[1])\n",
    "stats(y_base)\n",
    "y_gn, _ = tolerant_mean(rewards[2])\n",
    "stats(y_gn)\n",
    "plt.plot(np.arange(len(y_gn))+1, y_gn, color='red', label='Gaussian Noise')\n",
    "plt.plot(np.arange(len(y_ras))+1, y_ras, color='blue', label='Random Amplitude Scaling')\n",
    "plt.plot(np.arange(len(y_base))+1, y_base, color='green', label='Baseline')\n",
    "plt.xlabel('epsidoes')\n",
    "plt.ylabel('reward')\n",
    "plt.legend()\n",
    "plt.show()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
