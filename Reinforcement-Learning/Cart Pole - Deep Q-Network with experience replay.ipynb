{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "\n",
    "def stats(rewards):\n",
    "    print(\"Mean reward: \", np.mean(rewards))\n",
    "    rewards_100 = []\n",
    "    for i in range(100, len(rewards) + 1):\n",
    "        rewards_100.append(np.mean(rewards[i-100:i]))\n",
    "    #print(\"Max 100 rewards mean: \", np.max(rewards_100))\n",
    "    #re = np.argmax(rewards_100)\n",
    "    #print(\"Max 100 rewards from episode: %d, to episode: %d\" % (re, re + 99))\n",
    "    plt.plot(rewards_100)\n",
    "    plt.xlabel('episodes')\n",
    "    plt.ylabel('rewards')\n",
    "    plt.show()\n",
    "    \n",
    "def tolerant_mean(arrs):\n",
    "    lens = [len(i) for i in arrs]\n",
    "    arr = np.ma.empty((np.max(lens),len(arrs)))\n",
    "    arr.mask = True\n",
    "    for idx, l in enumerate(arrs):\n",
    "        arr[:len(l),idx] = l\n",
    "    return arr.mean(axis = -1), arr.std(axis=-1)\n",
    "    \n",
    "def train(num_iterations=10):\n",
    "    env = gym.make('FrozenLake-v0')\n",
    "    env._max_episode_steps = 1000\n",
    "    max_num_episodes = 2000\n",
    "    checkpoint = 400\n",
    "    replay_experience_maxlen = 50000\n",
    "    batch_size = 64\n",
    "    #num_iterations = 10\n",
    "    #final_episodes = []\n",
    "\n",
    "    learning_rate = 0.001\n",
    "    decay = 0.99\n",
    "    training_rewards = []\n",
    "        \n",
    "\n",
    "    augs = ['ras', None, 'gn']\n",
    "    final_rewards = []\n",
    "        \n",
    "    for aug in augs:\n",
    "        curr_rewards = []\n",
    "        final_episodes = []\n",
    "        for i in range(num_iterations):\n",
    "            print(\"----------------ITERATION: \", i, aug)\n",
    "            tf.compat.v1.reset_default_graph()\n",
    "            X = tf.placeholder(dtype=tf.float32, shape=(None, 4))\n",
    "            y = tf.placeholder(dtype=tf.float32, shape=(None, 2))\n",
    "            net = tf.contrib.layers.fully_connected(X, 15)\n",
    "            Q = tf.contrib.layers.fully_connected(net, 2, activation_fn=None)\n",
    "            mse = tf.contrib.losses.mean_squared_error(y, Q)\n",
    "            train_step = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(mse)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            sess = tf.InteractiveSession()\n",
    "            tf.global_variables_initializer().run()\n",
    "\n",
    "            # Initialize empty experiences\n",
    "            replay_experience = deque(maxlen=replay_experience_maxlen)\n",
    "            for episode in range(max_num_episodes):\n",
    "                state = env.reset()\n",
    "                epsilon = 1./((episode/50) + 10)\n",
    "                done = False\n",
    "                episode_reward = 0\n",
    "                while not done:\n",
    "                    # Calculate Q(s, a) for all a\n",
    "                    Q_s_A = Q.eval(feed_dict={X: state.reshape((1, 4))})\n",
    "\n",
    "                    # Choose action based on epsilon-greedy policy\n",
    "                    if np.random.random_sample() < epsilon:\n",
    "                        action = env.action_space.sample()\n",
    "                    else:\n",
    "                        action = np.argmax(Q_s_A[0])\n",
    "\n",
    "                    # Perform action\n",
    "                    next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "                    # Append final reward for each episode\n",
    "                    episode_reward += reward\n",
    "\n",
    "                    # Change 0 reward to -1 to learn more from punishment\n",
    "                    if done:\n",
    "                        reward = -1.0\n",
    "\n",
    "                    # Save experience\n",
    "                    replay_experience.append([state, action, reward, next_state, done])\n",
    "\n",
    "                    # Switch to next state\n",
    "                    state = next_state\n",
    "\n",
    "                    # Do training if replay_experience contains enough sample > batch_size\n",
    "                    if len(replay_experience) > batch_size:\n",
    "                        \n",
    "\n",
    "                        ## 1- Sample from replay experience\n",
    "                        batch = random.sample(replay_experience, batch_size)\n",
    "                        states = np.vstack([x[0] for x in batch])\n",
    "\n",
    "                        actions = np.array([x[1] for x in batch])\n",
    "                        rewards = np.array([x[2] for x in batch])\n",
    "                        next_states = np.vstack([x[3] for x in batch])\n",
    "                        if aug == 'ras':\n",
    "                            low = np.random.uniform(0.6, 0.8)\n",
    "                            high = np.random.uniform(1.2, 1.4)\n",
    "                            scaling = np.random.uniform(low, high)\n",
    "                            next_states *= scaling\n",
    "                            states *= scaling\n",
    "                        elif aug == 'gn':\n",
    "                            mean = np.mean(states)\n",
    "                            std = np.std(states)\n",
    "                            noise = np.random.normal(mean, std)\n",
    "                            states += noise\n",
    "                            next_states += noise\n",
    "\n",
    "                        episodes_done = np.array([x[4] for x in batch])\n",
    "                        target_Q = Q.eval(feed_dict={X: states})\n",
    "                        target_Q[range(batch_size), actions] = rewards + decay * np.max(Q.eval(feed_dict={X: next_states}), axis=1) * ~episodes_done\n",
    "                        train_step.run(feed_dict={X: states, y: target_Q})\n",
    "\n",
    "                if (episode + 1) % checkpoint == 0:\n",
    "                    print(\"Episode: %d\" % (episode))\n",
    "\n",
    "\n",
    "            print(\"Play 10 times with optimal policy\")\n",
    "            avg_rewards = []\n",
    "            for i in range(10):\n",
    "                state = env.reset()\n",
    "                done = False\n",
    "                total_reward = 0\n",
    "                while not done:\n",
    "                    state, reward, done, _ = env.step(np.argmax(Q.eval(feed_dict={X: state.reshape((1, 4))})))\n",
    "                    #print(state, reward, done)\n",
    "                    total_reward += reward\n",
    "                    #env.render()\n",
    "                avg_rewards.append(total_reward)\n",
    "                print(\"Iteration: %d, Total Reward: %d\" % (i, total_reward))\n",
    "            final_rewards.append([np.mean(avg_rewards), np.std(avg_rewards)])\n",
    "    return final_rewards\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------ITERATION:  0 ras\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From /Users/danielshvarts/anaconda3/lib/python3.7/site-packages/tensorflow_core/contrib/layers/python/layers/layers.py:1866: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From <ipython-input-1-ea21b6bbdc8f>:61: mean_squared_error (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.mean_squared_error instead.\n",
      "WARNING:tensorflow:From /Users/danielshvarts/anaconda3/lib/python3.7/site-packages/tensorflow_core/contrib/losses/python/losses/loss_ops.py:518: compute_weighted_loss (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.compute_weighted_loss instead.\n",
      "WARNING:tensorflow:From /Users/danielshvarts/anaconda3/lib/python3.7/site-packages/tensorflow_core/contrib/losses/python/losses/loss_ops.py:152: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "WARNING:tensorflow:From /Users/danielshvarts/anaconda3/lib/python3.7/site-packages/tensorflow_core/contrib/losses/python/losses/loss_ops.py:154: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /Users/danielshvarts/anaconda3/lib/python3.7/site-packages/tensorflow_core/contrib/losses/python/losses/loss_ops.py:121: add_loss (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.add_loss instead.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 1 into shape (1,4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-beb5c88e2a68>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-ea21b6bbdc8f>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(num_iterations)\u001b[0m\n\u001b[1;32m     78\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m                     \u001b[0;31m# Calculate Q(s, a) for all a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m                     \u001b[0mQ_s_A\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m                     \u001b[0;31m# Choose action based on epsilon-greedy policy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 1 into shape (1,4)"
     ]
    }
   ],
   "source": [
    "rewards = train(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[70.6, 49.70754469896899], [152.2, 4.833218389437828], [1000.0, 0.0], [1000.0, 0.0], [1000.0, 0.0], [9.8, 0.8717797887081348], [22.8, 10.85172797300043], [150.9, 10.367738422626218], [868.3, 218.65180081581767], [11.8, 0.8717797887081348], [158.5, 34.575280186861825], [147.0, 39.5145542806698], [140.4, 50.227880703848136], [186.9, 29.152872928752668], [122.4, 31.978742939646644]]\n",
      "15\n",
      "[  70.6  152.2 1000.  1000.  1000. ]\n",
      "644.5600000000001 436.0874297660963\n",
      "212.71999999999997 332.03066966772803\n",
      "151.04 21.407344534061203\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"y_ras, _ = tolerant_mean(rewards[0])\\nstats(y_ras)\\ny_base, _ = tolerant_mean(rewards[1])\\nstats(y_base)\\ny_gn, _ = tolerant_mean(rewards[2])\\nstats(y_gn)\\nplt.plot(np.arange(len(y_gn))+1, y_gn, color='red', label='Gaussian Noise')\\nplt.plot(np.arange(len(y_ras))+1, y_ras, color='blue', label='Random Amplitude Scaling')\\nplt.plot(np.arange(len(y_base))+1, y_base, color='green', label='Baseline')\\nplt.xlabel('epsidoes')\\nplt.ylabel('reward')\\nplt.legend()\\nplt.show()\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(rewards)\n",
    "print(len(rewards))\n",
    "\n",
    "np_rewards = np.array(rewards)\n",
    "\n",
    "print(np_rewards[:5,0])\n",
    "ras_mean = np.mean(np_rewards[:5,0])\n",
    "ras_std = np.std(np_rewards[:5,0])\n",
    "\n",
    "print(ras_mean, ras_std)\n",
    "\n",
    "base_mean = np.mean(np_rewards[5:10,0])\n",
    "base_std = np.std(np_rewards[5:10,0])\n",
    "\n",
    "print(base_mean, base_std)\n",
    "\n",
    "gn_mean = np.mean(np_rewards[10:,0])\n",
    "gn_std = np.std(np_rewards[10:,0])\n",
    "\n",
    "print(gn_mean, gn_std)\n",
    "\n",
    "'''y_ras, _ = tolerant_mean(rewards[0])\n",
    "stats(y_ras)\n",
    "y_base, _ = tolerant_mean(rewards[1])\n",
    "stats(y_base)\n",
    "y_gn, _ = tolerant_mean(rewards[2])\n",
    "stats(y_gn)\n",
    "plt.plot(np.arange(len(y_gn))+1, y_gn, color='red', label='Gaussian Noise')\n",
    "plt.plot(np.arange(len(y_ras))+1, y_ras, color='blue', label='Random Amplitude Scaling')\n",
    "plt.plot(np.arange(len(y_base))+1, y_base, color='green', label='Baseline')\n",
    "plt.xlabel('epsidoes')\n",
    "plt.ylabel('reward')\n",
    "plt.legend()\n",
    "plt.show()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
