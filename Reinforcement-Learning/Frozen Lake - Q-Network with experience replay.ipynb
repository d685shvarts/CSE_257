{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "%matplotlib inline\n",
    "\n",
    "id16 = np.identity(16)\n",
    "def onehot(x):\n",
    "    return id16[x : x + 1]\n",
    "\n",
    "def stats(rewards):\n",
    "    print(\"Mean reward: \", np.mean(rewards))\n",
    "    rewards_100 = []\n",
    "    for i in range(100, len(rewards) + 1):\n",
    "        rewards_100.append(np.mean(rewards[i-100:i]))\n",
    "    print(\"Max 100 rewards mean: \", np.max(rewards_100))\n",
    "    re = np.argmax(rewards_100)\n",
    "    print(\"Max 100 rewards from episode: %d, to episode: %d\" % (re - 100, re))\n",
    "    #plt.plot(rewards_100)\n",
    "    #plt.show()\n",
    "    \n",
    "def train(num_iterations=10):\n",
    "    \n",
    "\n",
    "    learning_rate = 0.001\n",
    "    decay = 0.99\n",
    "    augs = ['ras', None, 'gn']\n",
    "    final_rewards = []\n",
    "    \n",
    "    for aug in augs:\n",
    "        for i in range(num_iterations):\n",
    "            print(f\"---Iteration #{i} w/ {aug}\")\n",
    "            env = gym.make('FrozenLake-v0')\n",
    "            env._max_episode_steps = 10000\n",
    "            max_num_episodes = 5000\n",
    "            checkpoint = 200\n",
    "            replay_experience_maxlen = 50000\n",
    "            batch_size = 64\n",
    "            tf.reset_default_graph()\n",
    "            X = tf.placeholder(dtype=tf.float32, shape=(None, 4 * 4))\n",
    "            y = tf.placeholder(dtype=tf.float32, shape=(None, 4))\n",
    "            Q = tf.contrib.layers.fully_connected(X, 4, activation_fn=None)\n",
    "            mse = tf.contrib.losses.mean_squared_error(y, Q)\n",
    "            train_step = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(mse)\n",
    "\n",
    "            id16 = np.identity(16)\n",
    "            all_rewards = []\n",
    "\n",
    "            sess = tf.InteractiveSession()\n",
    "            tf.global_variables_initializer().run()\n",
    "\n",
    "            # Initialize empty experiences\n",
    "            replay_experience = deque(maxlen=replay_experience_maxlen)\n",
    "            for episode in range(max_num_episodes):\n",
    "                state = env.reset()\n",
    "                epsilon = 1./((episode/50) + 10)\n",
    "                done = False\n",
    "                while not done:\n",
    "                    # Calculate Q(s, a) for all a\n",
    "                    Q_s_A = Q.eval(feed_dict={X: onehot(state)})\n",
    "\n",
    "                    # Choose action based on epsilon-greedy policy\n",
    "                    if np.random.random_sample() < epsilon:\n",
    "                        action = env.action_space.sample()\n",
    "                    else:\n",
    "                        action = np.argmax(Q_s_A[0])\n",
    "\n",
    "                    # Perform action\n",
    "                    next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "                    # Append final reward for each episode\n",
    "                    if done:\n",
    "                        all_rewards.append(reward)\n",
    "\n",
    "                    # Change 0 reward to -1 to learn more from punishment\n",
    "                    if done and reward < 0.5:\n",
    "                        reward = -1.0\n",
    "\n",
    "                    # Save experience\n",
    "                    replay_experience.append([onehot(state), action, reward, onehot(next_state), done])\n",
    "\n",
    "                    # Switch to next state\n",
    "                    state = next_state\n",
    "\n",
    "                    # Do training if replay_experience contains enough sample > batch_size\n",
    "                    if len(replay_experience) > batch_size:\n",
    "                        ## 1- Sample from replay experience\n",
    "                        batch = random.sample(replay_experience, batch_size)\n",
    "                        states = np.vstack([x[0] for x in batch])\n",
    "                        actions = np.array([x[1] for x in batch])\n",
    "                        rewards = np.array([x[2] for x in batch])\n",
    "                        next_states = np.vstack([x[3] for x in batch])\n",
    "                        if aug == \"ras\":\n",
    "                            low = np.random.uniform(0.6, 0.8)\n",
    "                            high = np.random.uniform(1.2, 1.4)\n",
    "                            scaling = np.random.uniform(low, high)\n",
    "                            next_states *= scaling\n",
    "                            states *= scaling\n",
    "                        elif aug == 'gn':\n",
    "                            #print('gn')\n",
    "                            mean = np.mean(states)\n",
    "                            std = np.std(states)\n",
    "                            noise = np.random.normal(mean, std)\n",
    "                            states += noise\n",
    "                            next_states += noise\n",
    "                        episodes_done = np.array([x[4] for x in batch])\n",
    "                        target_Q = Q.eval(feed_dict={X: states})\n",
    "                        target_Q[range(batch_size), actions] = rewards + decay * np.max(Q.eval(feed_dict={X: next_states}), axis=1) * ~episodes_done\n",
    "                        train_step.run(feed_dict={X: states, y: target_Q})\n",
    "\n",
    "                if (episode + 1) % checkpoint == 0:\n",
    "                    print(\"Episode: \", episode, np.mean(all_rewards))\n",
    "\n",
    "                if episode == max_num_episodes - 1:# or (len(all_rewards) >= 100 and np.mean(all_rewards[-100:]) >= 0.75):\n",
    "                    #training_episodes.append(episode)\n",
    "                    break\n",
    "            print(\"Play 10 times with optimal policy\")\n",
    "            avg_rewards = []\n",
    "            for i in range(1000):\n",
    "                state = env.reset()\n",
    "                done = False\n",
    "                total_reward = 0\n",
    "                while not done:\n",
    "                    # Calculate Q(s, a) for all a\n",
    "                    Q_s_A = Q.eval(feed_dict={X: onehot(state)})\n",
    "\n",
    "                    # Choose action based on epsilon-greedy policy\n",
    "                    action = np.argmax(Q_s_A[0])\n",
    "                        \n",
    "                    next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "                    total_reward += reward\n",
    "                    # Switch to next state\n",
    "                    state = next_state\n",
    "                    #env.render()\n",
    "                avg_rewards.append(total_reward)\n",
    "                #print(\"Iteration: %d, Total Reward: %d\" % (i, total_reward))\n",
    "            print('AVG REWARD: ', np.mean(avg_rewards))\n",
    "            final_rewards.append(np.mean(avg_rewards))\n",
    "            \n",
    "            env.close()\n",
    "            stats(all_rewards)\n",
    "            \n",
    "    return final_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Iteration #0 w/ ras\n",
      "Episode:  199 0.05\n",
      "Episode:  399 0.1525\n",
      "Episode:  599 0.27666666666666667\n",
      "Episode:  799 0.35375\n",
      "Episode:  999 0.409\n",
      "Episode:  1199 0.43833333333333335\n",
      "Episode:  1399 0.4742857142857143\n",
      "Episode:  1599 0.500625\n",
      "Episode:  1799 0.5272222222222223\n",
      "Episode:  1999 0.543\n",
      "Episode:  2199 0.5563636363636364\n",
      "Episode:  2399 0.5675\n",
      "Episode:  2599 0.578076923076923\n",
      "Episode:  2799 0.5889285714285715\n",
      "Episode:  2999 0.5976666666666667\n",
      "Episode:  3199 0.6040625\n",
      "Episode:  3399 0.6111764705882353\n",
      "Episode:  3599 0.6163888888888889\n",
      "Episode:  3799 0.6228947368421053\n",
      "Episode:  3999 0.6305\n",
      "Episode:  4199 0.6390476190476191\n",
      "Episode:  4399 0.6427272727272727\n",
      "Episode:  4599 0.6460869565217391\n",
      "Episode:  4799 0.650625\n",
      "Episode:  4999 0.6524\n",
      "Play 10 times with optimal policy\n",
      "AVG REWARD:  0.844\n",
      "Mean reward:  0.6524\n",
      "Max 100 rewards mean:  0.9\n",
      "Max 100 rewards from episode: 3942, to episode: 4042\n",
      "---Iteration #1 w/ ras\n",
      "Episode:  199 0.085\n",
      "Episode:  399 0.2125\n",
      "Episode:  599 0.28833333333333333\n",
      "Episode:  799 0.335\n",
      "Episode:  999 0.379\n",
      "Episode:  1199 0.42\n",
      "Episode:  1399 0.45571428571428574\n",
      "Episode:  1599 0.471875\n",
      "Episode:  1799 0.49277777777777776\n",
      "Episode:  1999 0.512\n",
      "Episode:  2199 0.5281818181818182\n",
      "Episode:  2399 0.5458333333333333\n",
      "Episode:  2599 0.5626923076923077\n",
      "Episode:  2799 0.5757142857142857\n",
      "Episode:  2999 0.5886666666666667\n",
      "Episode:  3199 0.5965625\n",
      "Episode:  3399 0.6058823529411764\n",
      "Episode:  3599 0.615\n",
      "Episode:  3799 0.6186842105263158\n",
      "Episode:  3999 0.62475\n",
      "Episode:  4199 0.6307142857142857\n",
      "Episode:  4399 0.6363636363636364\n",
      "Episode:  4599 0.6397826086956522\n",
      "Episode:  4799 0.6454166666666666\n",
      "Episode:  4999 0.6512\n",
      "Play 10 times with optimal policy\n",
      "AVG REWARD:  0.819\n",
      "Mean reward:  0.6512\n",
      "Max 100 rewards mean:  0.85\n",
      "Max 100 rewards from episode: 2702, to episode: 2802\n",
      "---Iteration #2 w/ ras\n",
      "Episode:  199 0.06\n",
      "Episode:  399 0.1025\n",
      "Episode:  599 0.235\n",
      "Episode:  799 0.33\n",
      "Episode:  999 0.393\n",
      "Episode:  1199 0.43583333333333335\n",
      "Episode:  1399 0.47285714285714286\n",
      "Episode:  1599 0.501875\n",
      "Episode:  1799 0.5227777777777778\n",
      "Episode:  1999 0.5405\n",
      "Episode:  2199 0.5554545454545454\n",
      "Episode:  2399 0.5658333333333333\n",
      "Episode:  2599 0.5761538461538461\n",
      "Episode:  2799 0.5832142857142857\n",
      "Episode:  2999 0.5953333333333334\n",
      "Episode:  3199 0.60125\n",
      "Episode:  3399 0.6079411764705882\n",
      "Episode:  3599 0.6105555555555555\n",
      "Episode:  3799 0.6157894736842106\n",
      "Episode:  3999 0.6205\n",
      "Episode:  4199 0.6266666666666667\n",
      "Episode:  4399 0.6315909090909091\n",
      "Episode:  4599 0.6378260869565218\n",
      "Episode:  4799 0.6420833333333333\n",
      "Episode:  4999 0.6476\n",
      "Play 10 times with optimal policy\n",
      "AVG REWARD:  0.815\n",
      "Mean reward:  0.6476\n",
      "Max 100 rewards mean:  0.82\n",
      "Max 100 rewards from episode: 4198, to episode: 4298\n",
      "---Iteration #3 w/ ras\n",
      "Episode:  199 0.165\n",
      "Episode:  399 0.3225\n",
      "Episode:  599 0.39666666666666667\n",
      "Episode:  799 0.44625\n",
      "Episode:  999 0.493\n",
      "Episode:  1199 0.5141666666666667\n",
      "Episode:  1399 0.5264285714285715\n",
      "Episode:  1599 0.545\n",
      "Episode:  1799 0.5544444444444444\n",
      "Episode:  1999 0.57\n",
      "Episode:  2199 0.58\n",
      "Episode:  2399 0.59\n",
      "Episode:  2599 0.6003846153846154\n",
      "Episode:  2799 0.6114285714285714\n",
      "Episode:  2999 0.618\n",
      "Episode:  3199 0.6265625\n",
      "Episode:  3399 0.6285294117647059\n",
      "Episode:  3599 0.6327777777777778\n",
      "Episode:  3799 0.6373684210526316\n",
      "Episode:  3999 0.64025\n",
      "Episode:  4199 0.6454761904761904\n",
      "Episode:  4399 0.6488636363636363\n",
      "Episode:  4599 0.6547826086956522\n",
      "Episode:  4799 0.6604166666666667\n",
      "Episode:  4999 0.6658\n",
      "Play 10 times with optimal policy\n",
      "AVG REWARD:  0.821\n",
      "Mean reward:  0.6658\n",
      "Max 100 rewards mean:  0.89\n",
      "Max 100 rewards from episode: 4770, to episode: 4870\n",
      "---Iteration #4 w/ ras\n",
      "Episode:  199 0.305\n",
      "Episode:  399 0.38\n",
      "Episode:  599 0.4166666666666667\n",
      "Episode:  799 0.4325\n",
      "Episode:  999 0.47\n",
      "Episode:  1199 0.5091666666666667\n",
      "Episode:  1399 0.53\n",
      "Episode:  1599 0.541875\n",
      "Episode:  1799 0.5594444444444444\n",
      "Episode:  1999 0.5705\n",
      "Episode:  2199 0.5822727272727273\n",
      "Episode:  2399 0.595\n",
      "Episode:  2599 0.6042307692307692\n",
      "Episode:  2799 0.6089285714285714\n",
      "Episode:  2999 0.6183333333333333\n",
      "Episode:  3199 0.62625\n",
      "Episode:  3399 0.6344117647058823\n",
      "Episode:  3599 0.6380555555555556\n",
      "Episode:  3799 0.6431578947368422\n",
      "Episode:  3999 0.6485\n",
      "Episode:  4199 0.6526190476190477\n",
      "Episode:  4399 0.6561363636363636\n",
      "Episode:  4599 0.6602173913043479\n",
      "Episode:  4799 0.6641666666666667\n",
      "Episode:  4999 0.6676\n",
      "Play 10 times with optimal policy\n",
      "AVG REWARD:  0.83\n",
      "Mean reward:  0.6676\n",
      "Max 100 rewards mean:  0.85\n",
      "Max 100 rewards from episode: 4426, to episode: 4526\n",
      "---Iteration #0 w/ None\n",
      "Episode:  199 0.13\n",
      "Episode:  399 0.2475\n",
      "Episode:  599 0.31166666666666665\n",
      "Episode:  799 0.4075\n",
      "Episode:  999 0.463\n",
      "Episode:  1199 0.49583333333333335\n",
      "Episode:  1399 0.5207142857142857\n",
      "Episode:  1599 0.541875\n",
      "Episode:  1799 0.5583333333333333\n",
      "Episode:  1999 0.5715\n",
      "Episode:  2199 0.5859090909090909\n",
      "Episode:  2399 0.6004166666666667\n",
      "Episode:  2599 0.6069230769230769\n",
      "Episode:  2799 0.6157142857142858\n",
      "Episode:  2999 0.6223333333333333\n",
      "Episode:  3199 0.625\n",
      "Episode:  3399 0.6305882352941177\n",
      "Episode:  3599 0.6388888888888888\n",
      "Episode:  3799 0.6442105263157895\n",
      "Episode:  3999 0.652\n",
      "Episode:  4199 0.6561904761904762\n",
      "Episode:  4399 0.6597727272727273\n",
      "Episode:  4599 0.6641304347826087\n",
      "Episode:  4799 0.668125\n",
      "Episode:  4999 0.672\n",
      "Play 10 times with optimal policy\n",
      "AVG REWARD:  0.835\n",
      "Mean reward:  0.672\n",
      "Max 100 rewards mean:  0.86\n",
      "Max 100 rewards from episode: 3665, to episode: 3765\n",
      "---Iteration #1 w/ None\n",
      "Episode:  199 0.32\n",
      "Episode:  399 0.405\n",
      "Episode:  599 0.43166666666666664\n",
      "Episode:  799 0.48\n",
      "Episode:  999 0.509\n",
      "Episode:  1199 0.5333333333333333\n",
      "Episode:  1399 0.5492857142857143\n",
      "Episode:  1599 0.564375\n",
      "Episode:  1799 0.5777777777777777\n",
      "Episode:  1999 0.589\n",
      "Episode:  2199 0.5963636363636363\n",
      "Episode:  2399 0.6008333333333333\n",
      "Episode:  2599 0.6153846153846154\n",
      "Episode:  2799 0.6228571428571429\n",
      "Episode:  2999 0.6263333333333333\n",
      "Episode:  3199 0.63\n",
      "Episode:  3399 0.6367647058823529\n",
      "Episode:  3599 0.6419444444444444\n",
      "Episode:  3799 0.6492105263157895\n",
      "Episode:  3999 0.65475\n",
      "Episode:  4199 0.6633333333333333\n",
      "Episode:  4399 0.6672727272727272\n",
      "Episode:  4599 0.6719565217391305\n",
      "Episode:  4799 0.6739583333333333\n",
      "Episode:  4999 0.6752\n",
      "Play 10 times with optimal policy\n",
      "AVG REWARD:  0.833\n",
      "Mean reward:  0.6752\n",
      "Max 100 rewards mean:  0.88\n",
      "Max 100 rewards from episode: 3895, to episode: 3995\n",
      "---Iteration #2 w/ None\n",
      "Episode:  199 0.2\n",
      "Episode:  399 0.3475\n",
      "Episode:  599 0.395\n",
      "Episode:  799 0.43625\n",
      "Episode:  999 0.469\n",
      "Episode:  1199 0.505\n",
      "Episode:  1399 0.5292857142857142\n",
      "Episode:  1599 0.545\n",
      "Episode:  1799 0.5605555555555556\n",
      "Episode:  1999 0.571\n",
      "Episode:  2199 0.5795454545454546\n",
      "Episode:  2399 0.59125\n",
      "Episode:  2599 0.6038461538461538\n",
      "Episode:  2799 0.6157142857142858\n",
      "Episode:  2999 0.627\n",
      "Episode:  3199 0.6334375\n",
      "Episode:  3399 0.6385294117647059\n",
      "Episode:  3599 0.6452777777777777\n",
      "Episode:  3799 0.6536842105263158\n",
      "Episode:  3999 0.65575\n",
      "Episode:  4199 0.6611904761904762\n",
      "Episode:  4399 0.6659090909090909\n",
      "Episode:  4599 0.6689130434782609\n",
      "Episode:  4799 0.6708333333333333\n",
      "Episode:  4999 0.674\n",
      "Play 10 times with optimal policy\n",
      "AVG REWARD:  0.833\n",
      "Mean reward:  0.674\n",
      "Max 100 rewards mean:  0.86\n",
      "Max 100 rewards from episode: 2215, to episode: 2315\n",
      "---Iteration #3 w/ None\n",
      "Episode:  199 0.215\n",
      "Episode:  399 0.3175\n",
      "Episode:  599 0.41333333333333333\n",
      "Episode:  799 0.46\n",
      "Episode:  999 0.49\n",
      "Episode:  1199 0.5083333333333333\n",
      "Episode:  1399 0.5414285714285715\n",
      "Episode:  1599 0.56\n",
      "Episode:  1799 0.5766666666666667\n",
      "Episode:  1999 0.5915\n",
      "Episode:  2199 0.600909090909091\n",
      "Episode:  2399 0.6129166666666667\n",
      "Episode:  2599 0.6203846153846154\n",
      "Episode:  2799 0.6307142857142857\n",
      "Episode:  2999 0.6396666666666667\n",
      "Episode:  3199 0.645625\n",
      "Episode:  3399 0.6508823529411765\n",
      "Episode:  3599 0.6569444444444444\n",
      "Episode:  3799 0.6610526315789473\n",
      "Episode:  3999 0.667\n",
      "Episode:  4199 0.6695238095238095\n",
      "Episode:  4399 0.673409090909091\n",
      "Episode:  4599 0.675\n",
      "Episode:  4799 0.675\n",
      "Episode:  4999 0.6784\n",
      "Play 10 times with optimal policy\n",
      "AVG REWARD:  0.837\n",
      "Mean reward:  0.6784\n",
      "Max 100 rewards mean:  0.85\n",
      "Max 100 rewards from episode: 3555, to episode: 3655\n",
      "---Iteration #4 w/ None\n",
      "Episode:  199 0.285\n",
      "Episode:  399 0.4\n",
      "Episode:  599 0.45166666666666666\n",
      "Episode:  799 0.48625\n",
      "Episode:  999 0.511\n",
      "Episode:  1199 0.525\n",
      "Episode:  1399 0.54\n",
      "Episode:  1599 0.5525\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:  1799 0.5761111111111111\n",
      "Episode:  1999 0.5905\n",
      "Episode:  2199 0.6\n",
      "Episode:  2399 0.61125\n",
      "Episode:  2599 0.6184615384615385\n",
      "Episode:  2799 0.6282142857142857\n",
      "Episode:  2999 0.6376666666666667\n",
      "Episode:  3199 0.6459375\n",
      "Episode:  3399 0.6494117647058824\n",
      "Episode:  3599 0.6561111111111111\n",
      "Episode:  3799 0.6623684210526316\n",
      "Episode:  3999 0.66675\n",
      "Episode:  4199 0.6735714285714286\n",
      "Episode:  4399 0.6768181818181818\n",
      "Episode:  4599 0.6789130434782609\n",
      "Episode:  4799 0.68375\n",
      "Episode:  4999 0.6848\n",
      "Play 10 times with optimal policy\n",
      "AVG REWARD:  0.744\n",
      "Mean reward:  0.6848\n",
      "Max 100 rewards mean:  0.85\n",
      "Max 100 rewards from episode: 3931, to episode: 4031\n",
      "---Iteration #0 w/ gn\n",
      "Episode:  199 0.08\n",
      "Episode:  399 0.125\n",
      "Episode:  599 0.11166666666666666\n",
      "Episode:  799 0.1075\n",
      "Episode:  999 0.168\n",
      "Episode:  1199 0.24\n",
      "Episode:  1399 0.29\n",
      "Episode:  1599 0.33875\n",
      "Episode:  1799 0.3761111111111111\n",
      "Episode:  1999 0.408\n",
      "Episode:  2199 0.43\n",
      "Episode:  2399 0.45375\n",
      "Episode:  2599 0.46923076923076923\n",
      "Episode:  2799 0.4903571428571429\n",
      "Episode:  2999 0.5053333333333333\n",
      "Episode:  3199 0.51875\n",
      "Episode:  3399 0.5338235294117647\n",
      "Episode:  3599 0.5452777777777778\n",
      "Episode:  3799 0.5547368421052632\n",
      "Episode:  3999 0.56425\n",
      "Episode:  4199 0.5714285714285714\n",
      "Episode:  4399 0.5804545454545454\n",
      "Episode:  4599 0.5876086956521739\n",
      "Episode:  4799 0.5941666666666666\n",
      "Episode:  4999 0.6012\n",
      "Play 10 times with optimal policy\n",
      "AVG REWARD:  0.82\n",
      "Mean reward:  0.6012\n",
      "Max 100 rewards mean:  0.85\n",
      "Max 100 rewards from episode: 3243, to episode: 3343\n",
      "---Iteration #1 w/ gn\n",
      "Episode:  199 0.025\n",
      "Episode:  399 0.1075\n",
      "Episode:  599 0.225\n",
      "Episode:  799 0.27125\n",
      "Episode:  999 0.308\n",
      "Episode:  1199 0.3591666666666667\n",
      "Episode:  1399 0.395\n",
      "Episode:  1599 0.426875\n",
      "Episode:  1799 0.46\n",
      "Episode:  1999 0.485\n",
      "Episode:  2199 0.5059090909090909\n",
      "Episode:  2399 0.5208333333333334\n",
      "Episode:  2599 0.5353846153846153\n",
      "Episode:  2799 0.5525\n",
      "Episode:  2999 0.5616666666666666\n",
      "Episode:  3199 0.576875\n",
      "Episode:  3399 0.5852941176470589\n",
      "Episode:  3599 0.5944444444444444\n",
      "Episode:  3799 0.6052631578947368\n",
      "Episode:  3999 0.611\n",
      "Episode:  4199 0.6188095238095238\n",
      "Episode:  4399 0.625\n",
      "Episode:  4599 0.6306521739130435\n",
      "Episode:  4799 0.6335416666666667\n",
      "Episode:  4999 0.6384\n",
      "Play 10 times with optimal policy\n",
      "AVG REWARD:  0.817\n",
      "Mean reward:  0.6384\n",
      "Max 100 rewards mean:  0.87\n",
      "Max 100 rewards from episode: 3526, to episode: 3626\n",
      "---Iteration #2 w/ gn\n",
      "Episode:  199 0.035\n",
      "Episode:  399 0.03\n",
      "Episode:  599 0.035\n",
      "Episode:  799 0.03875\n",
      "Episode:  999 0.045\n",
      "Episode:  1199 0.0475\n",
      "Episode:  1399 0.06571428571428571\n",
      "Episode:  1599 0.08\n",
      "Episode:  1799 0.08666666666666667\n",
      "Episode:  1999 0.092\n",
      "Episode:  2199 0.09954545454545455\n",
      "Episode:  2399 0.11375\n",
      "Episode:  2599 0.125\n",
      "Episode:  2799 0.13107142857142856\n",
      "Episode:  2999 0.14733333333333334\n",
      "Episode:  3199 0.180625\n",
      "Episode:  3399 0.20794117647058824\n",
      "Episode:  3599 0.23527777777777778\n",
      "Episode:  3799 0.25763157894736843\n",
      "Episode:  3999 0.2785\n",
      "Episode:  4199 0.30214285714285716\n",
      "Episode:  4399 0.32431818181818184\n",
      "Episode:  4599 0.34630434782608693\n",
      "Episode:  4799 0.36375\n",
      "Episode:  4999 0.3782\n",
      "Play 10 times with optimal policy\n",
      "AVG REWARD:  0.738\n",
      "Mean reward:  0.3782\n",
      "Max 100 rewards mean:  0.88\n",
      "Max 100 rewards from episode: 4280, to episode: 4380\n",
      "---Iteration #3 w/ gn\n",
      "Episode:  199 0.0\n",
      "Episode:  399 0.005\n",
      "Episode:  599 0.011666666666666667\n",
      "Episode:  799 0.0225\n",
      "Episode:  999 0.026\n",
      "Episode:  1199 0.028333333333333332\n",
      "Episode:  1399 0.054285714285714284\n",
      "Episode:  1599 0.1225\n",
      "Episode:  1799 0.17333333333333334\n",
      "Episode:  1999 0.2165\n",
      "Episode:  2199 0.2559090909090909\n",
      "Episode:  2399 0.29583333333333334\n",
      "Episode:  2599 0.3296153846153846\n",
      "Episode:  2799 0.35678571428571426\n",
      "Episode:  2999 0.38166666666666665\n",
      "Episode:  3199 0.40375\n",
      "Episode:  3399 0.42323529411764704\n",
      "Episode:  3599 0.4388888888888889\n",
      "Episode:  3799 0.4544736842105263\n",
      "Episode:  3999 0.46925\n",
      "Episode:  4199 0.4828571428571429\n",
      "Episode:  4399 0.4968181818181818\n",
      "Episode:  4599 0.5058695652173913\n",
      "Episode:  4799 0.51625\n",
      "Episode:  4999 0.5238\n",
      "Play 10 times with optimal policy\n",
      "AVG REWARD:  0.735\n",
      "Mean reward:  0.5238\n",
      "Max 100 rewards mean:  0.84\n",
      "Max 100 rewards from episode: 4154, to episode: 4254\n",
      "---Iteration #4 w/ gn\n",
      "Episode:  199 0.375\n",
      "Episode:  399 0.43\n",
      "Episode:  599 0.4716666666666667\n",
      "Episode:  799 0.50625\n",
      "Episode:  999 0.531\n",
      "Episode:  1199 0.545\n",
      "Episode:  1399 0.5621428571428572\n",
      "Episode:  1599 0.575\n",
      "Episode:  1799 0.5855555555555556\n",
      "Episode:  1999 0.595\n",
      "Episode:  2199 0.6036363636363636\n",
      "Episode:  2399 0.6104166666666667\n",
      "Episode:  2599 0.6161538461538462\n",
      "Episode:  2799 0.6232142857142857\n",
      "Episode:  2999 0.6293333333333333\n",
      "Episode:  3199 0.6359375\n",
      "Episode:  3399 0.64\n",
      "Episode:  3599 0.6488888888888888\n",
      "Episode:  3799 0.6539473684210526\n",
      "Episode:  3999 0.657\n",
      "Episode:  4199 0.6592857142857143\n",
      "Episode:  4399 0.6627272727272727\n",
      "Episode:  4599 0.6678260869565218\n",
      "Episode:  4799 0.6708333333333333\n",
      "Episode:  4999 0.6738\n",
      "Play 10 times with optimal policy\n",
      "AVG REWARD:  0.8\n",
      "Mean reward:  0.6738\n",
      "Max 100 rewards mean:  0.82\n",
      "Max 100 rewards from episode: 3282, to episode: 3382\n",
      "Base stats:  0.8164 0.0362303739975176\n",
      "RAS stats:  0.8257999999999999 0.010342146779078326\n",
      "GN stats:  0.782 0.03778359432346265\n"
     ]
    }
   ],
   "source": [
    "num_iterations = 5\n",
    "episodes  = train(num_iterations)\n",
    "ras_mean = np.mean(episodes[:num_iterations]) \n",
    "ras_std = np.std(episodes[:num_iterations]) \n",
    "base_mean = np.mean(episodes[num_iterations:2*num_iterations]) \n",
    "base_std = np.std(episodes[num_iterations:2*num_iterations])\n",
    "gn_mean = np.mean(episodes[2*num_iterations:]) \n",
    "gn_std = np.std(episodes[2*num_iterations:])\n",
    "print(\"Base stats: \", base_mean, base_std)\n",
    "print(\"RAS stats: \", ras_mean, ras_std)\n",
    "print(\"GN stats: \", gn_mean, gn_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
