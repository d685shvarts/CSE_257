{"cells": [{"cell_type": "code", "execution_count": 3, "metadata": {}, "outputs": [{"ename": "AttributeError", "evalue": "module 'tensorflow' has no attribute 'placeholder'", "output_type": "error", "traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)", "\u001b[0;32m<ipython-input-3-86a0b7245800>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfully_connected\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'placeholder'"]}], "source": "import numpy as np\nimport tensorflow as tf\nimport gym\nimport random\nimport matplotlib.pyplot as plt\nfrom collections import deque\n\n%matplotlib inline\n\nenv = gym.make('CartPole-v0')\nmax_num_episodes = 2000\ncheckpoint = 100\nreplay_experience_maxlen = 50000\nbatch_size = 64\n\nlearning_rate = 0.001\ndecay = 0.99\n\ntf.compat.v1.reset_default_graph()\nX = tf.compat.v1.placeholder(dtype=tf.float32, shape=(None, 4))\ny = tf.compat.v1.placeholder(dtype=tf.float32, shape=(None, 2))\nnet = tf.contrib.layers.fully_connected(X, 15)\nQ = tf.contrib.layers.fully_connected(net, 2, activation_fn=None)\nmse = tf.contrib.losses.mean_squared_error(y, Q)\ntrain_step = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate).minimize(mse)\n\nall_rewards = []\n\ndef stats(rewards):\n    print(\"Mean reward: \", np.mean(rewards))\n    rewards_100 = []\n    for i in range(100, len(rewards) + 1):\n        rewards_100.append(np.mean(rewards[i-100:i]))\n    print(\"Max 100 rewards mean: \", np.max(rewards_100))\n    re = np.argmax(rewards_100)\n    print(\"Max 100 rewards from episode: %d, to episode: %d\" % (re, re + 99))\n    plt.plot(rewards_100)\n    plt.show()\n\nsess = tf.compat.v1.InteractiveSession()\ntf.compat.v1.global_variables_initializer().run()\n\n# Initialize empty experiences\nreplay_experience = deque(maxlen=replay_experience_maxlen)\nfor episode in range(max_num_episodes):\n    state = env.reset()\n    epsilon = 1./((episode/50) + 10)\n    done = False\n    episode_reward = 0\n    while not done:\n        # Calculate Q(s, a) for all a\n        Q_s_A = Q.eval(feed_dict={X: state.reshape((1, 4))})\n\n        # Choose action based on epsilon-greedy policy\n        if np.random.random_sample() < epsilon:\n            action = env.action_space.sample()\n        else:\n            action = np.argmax(Q_s_A[0])\n\n        # Perform action\n        next_state, reward, done, _ = env.step(action)\n\n        # Append final reward for each episode\n        episode_reward += reward\n\n        # Change 0 reward to -1 to learn more from punishment\n        if done:\n            reward = -1.0\n\n        # Save experience\n        replay_experience.append([state, action, reward, next_state, done])\n\n        # Switch to next state\n        state = next_state\n\n        # Do training if replay_experience contains enough sample > batch_size\n        if len(replay_experience) > batch_size:\n            ## 1- Sample from replay experience\n            batch = random.sample(replay_experience, batch_size)\n            states = np.vstack([x[0] for x in batch])\n            actions = np.array([x[1] for x in batch])\n            rewards = np.array([x[2] for x in batch])\n            next_states = np.vstack([x[3] for x in batch])\n            episodes_done = np.array([x[4] for x in batch])\n            target_Q = Q.eval(feed_dict={X: states})\n            target_Q[range(batch_size), actions] = rewards + decay * np.max(Q.eval(feed_dict={X: next_states}), axis=1) * ~episodes_done\n            train_step.run(feed_dict={X: states, y: target_Q})\n\n    if (episode + 1) % checkpoint == 0:\n        print(\"Episode: %d, Mean reward: %d\" % (episode, np.mean(all_rewards[-100:])))\n\n    all_rewards.append(episode_reward)\n\n    if np.mean(all_rewards[-100:]) > 199:\n        break\n\nstats(all_rewards)\n\nprint(\"Play 20 times with optimal policy\")\nfor i in range(20):\n    state = env.reset()\n    done = False\n    total_reward = 0\n    while not done:\n        state, reward, done, _ = env.step(np.argmax(Q.eval(feed_dict={X: state.reshape((1, 4))})))\n        total_reward += reward\n        env.render()\n    print(\"Iteration: %d, Total Reward: %d\" % (i, total_reward))\n\nsess.close()\nenv.close()"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.10"}}, "nbformat": 4, "nbformat_minor": 1}